{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "In this lab we'll look at fitting a model to predict a numerical outcome using one predictor.  We'll look at an example using a numerical predictor, and another example using a categorical predictor.\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "\n",
    "### Table of Contents\n",
    "- [Example 1: Age and Income](#ex1)\n",
    "    - [Preliminary Inspection](#prelim1)\n",
    "    - [Model Fitting](#modfit1)\n",
    "    - [Checking Assumptions](#assump1)\n",
    "    - [Effect Size](#eff1)\n",
    "- [Example 2: Gender and Income](#ex2)\n",
    "    - [Preliminary Inspection](#prelim2)\n",
    "    - [Model Fitting](#modfit2)\n",
    "    - [Checking Assumptions](#assump2)\n",
    "    - [Effect Size](#eff2)\n",
    "- [OPTIONAL: Special Model Fit Dashboard - `performance` package](#perf)\n",
    "\n",
    "We'll use the same data from the Cards Against Humanity poll that we've used in previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading some libraries!\n",
    "library(tidyverse) ## all of our normal functions for working with data\n",
    "library(olsrr) ## ols plots\n",
    "library(performance) ## OPTIONAL\n",
    "library(magrittr)\n",
    "\n",
    "options(repr.plot.width=10, repr.plot.height=8) ## set options for plot size within the notebook -\n",
    "# this is only for jupyter notebooks, you can disregard this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD the DATA\n",
    "cah <- read_csv(\"201806-CAH_PulseOfTheNation_Raw.csv\")\n",
    "## variable names currently full questions - need to rename\n",
    "new_names <- c(\"gender\", \"age\", \"agerange\", \"race\", \"income\", \"educ\", \"partyid\", \"polaffil\", \n",
    "               \"trump\", \"hollymoney\", \"fed_min_is\", \"fed_min_should\", \"fed_tax_is\", \"fed_tax_should\", \n",
    "               \"redist\", \"redist_you\", \"redist_people\", \"baseincome\", \"faircomp\", \"ceofair\", \"attractive\")\n",
    "colnames(cah) <- new_names\n",
    "cah %<>% drop_na(income, age)\n",
    "glimpse(cah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ex1\"></a>\n",
    "## Example 1: Predicting Income using Age\n",
    "In this first example we'll see if age is a good predictor of a person's income.\n",
    "\n",
    "### Preliminary Inspection\n",
    "Let's plot a scatterplot of age and income and see what linear relationship may exist.\n",
    "\n",
    "Unlike when we looked at correlations in the last lab, it now matters which is the x and which is the y variable.  The x variable should be your predictor (in this example, age) and the y variable should be your outcome (income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cah %>% ggplot(aes(x = age, y = income)) +\n",
    "            geom_point() +\n",
    "            geom_smooth(method = \"lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preliminary plot makes it look like there's not a lot going on.  Let's do a few things to improve this plot.\n",
    "\n",
    "1. Remove outliers - cap income at 200k as in previous labs.\n",
    "2. Use income/1000 on the y-axis to make labels more interpretable.\n",
    "3. Improve the plot to be fully PQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cah %<>% filter(income < 200000)\n",
    "\n",
    "cah %>% ggplot(aes(x = age, y = income/1000)) +\n",
    "            geom_point() +\n",
    "            geom_smooth(method = \"lm\") +\n",
    "            labs(x = \"Age\",\n",
    "                 y = \"Income in $1000s\",\n",
    "                 title = \"Relationship between Age and Income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty clear there's likely nothing going on here, but let's fit the model to check.\n",
    "\n",
    "NOTE - this graph starts at age == 18, because it's a survey of adults, so the y-intercept is not actually shown on the scatterplot.  If you wanted to extend the graph to include x == 0 you could use the ggplot option `xlim(0,0)`\n",
    "\n",
    "[Return to Top](#top)\n",
    "<a id = \"modfit1\"></a>\n",
    "\n",
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is fitted using lm function\n",
    "# lm(outcome ~ predictor, data = yourdf)\n",
    "mod1 <- lm(income ~ age, data = cah)\n",
    "\n",
    "# use summary on saved model output to see the results\n",
    "summary(mod1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "\n",
    "#### The intercept:\n",
    "This is a survey of adults 18 and older, so age == 0 is not possible.  Therefore the intercept is not interpretable.\n",
    "\n",
    "#### The coefficient estimte for age:\n",
    "Looking first at the p-value (last column) we can see that p > alpha (0.05), therefore we fail to reject null.  This means that the coefficient for age is **not** significantly different from zero.  This is that horizontal line (slope == 0) that we saw on the scatterplot.\n",
    "\n",
    "The coefficient for age reflects the change in the y-variable (income) with an increase of one year in age.  So the coefficient is in units of y (dollars).  It's not surprising that $14 is not significantly different from zero.\n",
    "\n",
    "Even though this is not significant, the proper interpretation of the coefficient estimate here is:\n",
    "\n",
    "**A one year increase in age is associated with a $14 increase in income.**\n",
    "\n",
    "Note that the increase is always one unit of x (here age is in years) and the yield of y is reflected by the coefficient estimate in units of y (dollars).\n",
    "\n",
    "#### R-squared\n",
    "R-squared is negative.  R-squared technically shouldn't be negative - it's the square of correlation and therefore should be constrained to between 0 and 1.  However, the R-squared is soooooooooo low that the adjustment to not overstate the r-squared in the population makes it below zero.\n",
    "\n",
    "#### F-statistic\n",
    "The F-statistic is our overall (omnibus) test of model fit.  It tells us:\n",
    "1. If our r-squared is significantly different from 0. AND\n",
    "2. If our model is significantly better than a \"null\" / \"empty\" / intercept only model.\n",
    "\n",
    "For this model our p-value is greater than alpha, therefore the model is not an acceptable model for predicting age, and as we could tell from the negative value of r-squared, r-squared is not significantly different from zero.\n",
    "\n",
    "*Because we have only one predictor variable (parameter), the p-value for the t-test of the coefficient and for the F-test of the model are identical.*\n",
    "\n",
    "[Return to Top](#top)\n",
    "<a id = \"assump1\"></a>\n",
    "\n",
    "### Checking Assumptions\n",
    "Even thought this model is not predictive of income, we still need to review the assumptions.  We might find issues with assumption violations that affect our model fit.\n",
    "\n",
    "#### Linear in the Parameters / Errors are Independent\n",
    "The assumption of linear in the parameters says that the variables are entered into the model correctly (that the relationships are truly linear.  This is not something we can test, but something we need to confirm as modelers.  However, if the errors are not independent, and there is a curvilinear relationship, that may be an indication that we possibly need to square one of our predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-PQ / quick and dirty plot\n",
    "# note - we're plotting with our saved model output - mod1\n",
    "plot(mod1, which = 1) ## residuals vs. fitted is plot #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq version of resid vs. fit - ggplot\n",
    "\n",
    "# First, save residuals and fitted values from model output to dataframe\n",
    "resfit <- data.frame(resid = mod1$residuals, \n",
    "                     fitted = mod1$fitted.values)\n",
    "\n",
    "#plot with ggplot\n",
    "resfit %>% ggplot(aes(x = fitted, y = resid)) +\n",
    "            geom_point() +\n",
    "            geom_smooth(color = \"red\", se = FALSE) + \n",
    "            ## do not use method = \"lm\" - we want to see possible curvilinear relationships\n",
    "            ## se = FALSE because we don't need CI around line.\n",
    "            labs(x = \"Fitted Values\",\n",
    "                 y = \"Residuals\",\n",
    "                 title = \"Model One - Residuals vs. Fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we would look for any apparent linear or curvilinear relationship between the fitted values (y_hat) and the residuals.  If there is no relationship, the red \"guide line\" would be horizontal.  In this case there is a slight curve (very slight curve) perhaps indicating that age has a curvilinear relationship with income (which may make sense - young people have low income and older people as the retire might not earn as much.  So we could refit our model to see if including age^2 improves the fit of the model.  But first let's evaluate the rest of the assumptions.\n",
    "\n",
    "#### Right variables \n",
    "Like linear in the parameters, it relies on the analyst to make sure they're using the right variables in the model.\n",
    "\n",
    "#### Normally Distributed Errors\n",
    "This we're already familiar with from ANOVA - we look at the normality of the distribution of the residuals using a QQ plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty QQ\n",
    "plot(mod1, which = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PQ QQplot\n",
    "\n",
    "# we can use the previously saved df with residuals included\n",
    "resfit %>% ggplot(aes(sample = resid)) +\n",
    "  geom_qq_line(color = \"red\", size = 1) +\n",
    "  geom_qq(color = \"black\") +\n",
    "  labs(title = \"QQ Plot of Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some moderate deviations from normality in the upper and lower tails.  This shape (tails above line) indicates that the distribution is right skewed.  Because the distribution of income is typically right skewed (long tail in the upper income levels) this is not surprising.  But should be noted as a violation of the assumption of normally distributed errors.\n",
    "\n",
    "#### No influential outliers\n",
    "We need to make sure that we don't have any observations that 1. Have inordinate influence on the fit of the line and 2. are outliers.  For this week look at a plot of Residuals vs. Leverage and/or Cook's Distance (Cook's D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty plot\n",
    "plot(mod1, which = 5) ## plot 5 is Residuals vs. Leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any obs that exceed Cook's D threshold 0.5\n",
    "cd <- cooks.distance(mod1)\n",
    "lev <- cd > 0.5\n",
    "cd[lev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PQ plot, OLSRR version\n",
    "ols_plot_resid_lev(mod1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quick and dirty plot uses a threshold of 0.5, therefore does not indicate that there are any influential outliers.  The same thing with running the code looking for any observation with Cook's D higher than 0.5.  OLSRR uses a different threshold, and therefore identifies two possible influential outliers - observations 235 and 183.  Note that the numbers included on all of the plots are the observation number (the row number) in the dataframe of the potential influential outlier.  We can address this when we refit the model.\n",
    "\n",
    "#### Homoscedasticity / Homoskedasticity\n",
    "Finally, we turn to our last assumption, Homoscedasticity.  This is the assumption of constant variance in x over the range of y.  If we violate this assumption, that condition is called Heteroscedasticity.  We can examine this using a scale-location plot, a plot of fitted vs. residuals (same as above with Independent Errors) and by running the Breusch Pagan Test of Heteroskedasticity (a statistical test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty plots\n",
    "plot(mod1, which = c(1,3)) ## residuals vs. fitted is plot #1, scale-location is plot 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both of these, we're looking for a situation where the dots form a funnel or cone shape.  If we have constant variance (homoscedasticity) the resid/fitted points will form roughly a rectangle shape around the horizontal midline.  In this case, any potential shape is not readily apparent.  We can confirm with the Breush-Pagan Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_test_breusch_pagan(mod1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our p-value is greater than an alpha of 0.5, which confirms that we do not violate the assumption of constant variance.  So we're good here - we have homoscedasticity.\n",
    "\n",
    "PQ graph - you can use the same ggplot code as we did in the first block with Independent Errors - they both use residuals vs. fitted.\n",
    "\n",
    "NOTE: Even if you reject null on Breusch Pagan test I would still look on the plot to see how much it looks cone/funnel shaped.  The B-P test is VERY sensitive.\n",
    "\n",
    "### Refitting the Model\n",
    "We made two observations in reviewing our assumptions/model fit that we could potentially \"fix\" to see if it improves the model.\n",
    "\n",
    "1. Use age-squared (age^2) instead of age to address possible curvilinear relationship.\n",
    "2. Remove influential outliers.\n",
    "\n",
    "Let's prepare a copy of the data and see if the model fit is improved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add age^2 as a variable\n",
    "cah2 <- cah %>% mutate(agesq = age^2)\n",
    "\n",
    "# influential outliers (by reviewing OLSRR plot) are 183 and 235\n",
    "# remove these from df\n",
    "\n",
    "cah2 <- cah2[-c(183,235), ] # remove rows by row number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit model\n",
    "# include both age and agesq as predictors\n",
    "mod2 <- lm(income ~ age + agesq, data = cah2)\n",
    "summary(mod2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW!  That really improved our model fit.\n",
    "\n",
    "1. Age and Age-squared are both significant predictors of age.  The coefficient of age describes the linear relationship between age and income, and the coefficient for age-squared defines the curvilinear relationship.  They are interpreted separately (now we actually have two predictors/parameters in our model).\n",
    "    - With each year increase in age, income increases by about 2000 dollars.\n",
    "    - With each year increase in *squared age*, income decreases by $19.  This is small, but significant.  Squared age is not interpretable as a unit, however adding this accounts for the decrease in income among the oldest respondents (and separates that out from the linear effect of age).\n",
    "    \n",
    "2. The p-value for the F-test of the overall model fit is less than alpha.  So our model is now significantly better than a null or empty model.  This indicates that we have an acceptable model with some predictive power.\n",
    "\n",
    "3. The r-squared is 0.03.  This means that 3% of the variance in income is explained by the model (combination of age and squared age).  This is not a huge amount, but based on the F-test, we know it's significantly different from zero.\n",
    "\n",
    "4. The intercept is still irrelevant.\n",
    "\n",
    "Reviewing our plots for our assumptions:\n",
    "\n",
    "This time I'm just going to run plot on the model and let the 4 default plots print.  These will cover the ones we need to review. (reminder - these are not pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(mod2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors appear to be independent, there residuals are somewhat normal (no change from previous), no influential outliers, and no clear violation of constant variance is evident.  So we got a good model, with significant but not strong predictive power.\n",
    "\n",
    "Which leads us to...\n",
    "\n",
    "[Return to Top](#top)\n",
    "<a id = \"eff1\"></a>\n",
    "\n",
    "### Effect Size\n",
    "\n",
    "#### Unstandardized Effect Size\n",
    "The unstandardized effect size is the value of the coefficient(s) in units of Y (in our example here, income).  The increase of 1 year of age yields an increase of $2000 in income.  We need to decide if we think this is large, or not.\n",
    "\n",
    "#### Standardized Effect Size\n",
    "We can't directly compare coefficients with different units of x.  A difference of 1 year of age yielding an increase of $2000 in income cannot be directly compared to an increase of 1 month of work experience yielding a smaller increase in income - the one unit increase of the x value is not directly comparable.  \n",
    "\n",
    "But, if we standardize the coefficients to constrain our means to 0 and our standard deviations to 1, the units are then standard deviations (z-scores).  This makes it harder to understand the interpretation of coefficients (especially for a non-statistician), but allows us to directly compare the magnitude of the different coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize variables using scale()\n",
    "\n",
    "cah2 %<>% mutate(age_std = scale(age),\n",
    "                 income_std = scale(income),\n",
    "                 agesq_std = scale(agesq))\n",
    "\n",
    "# refit the model\n",
    "mod3 <- lm(income_std ~ age_std + agesq_std, data = cah2)\n",
    "summary(mod3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this doesn't change any of the p-values, it only changes our coefficient estimates.  \n",
    "1. A one standard deviation increase in age yields a 1 standard deviation (1.04 SD) increase in income.\n",
    "2. A one standard deviation increase in squared age yields a 1 standard deviation **decrease** (-1.02) in income.\n",
    "\n",
    "#### R-squared\n",
    "R-squared is still an effect size we discuss.  Again, it's the proportion of variance in our outcome that is explained by the model.  We interpreted that in the model interpretation above - this model only explains 3% of the variance in income, so although the model is significant, the predictive power of this model is poor.\n",
    "\n",
    "[Return to Top](#top)\n",
    "<a id = \"ex2\"></a>\n",
    "\n",
    "## Example 2: Predicting Income using Gender\n",
    "Now we'll see if gender is a good predictor of a person's income.\n",
    "\n",
    "<a id = \"prelim2\"></a>\n",
    "### Preliminary Inspection\n",
    "Because we have a categorical predictor, we cannot use a scatterplot to preview the relationship between the variables.  But let's peek at a boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "cah %<>% filter(gender %in% c(\"Male\", \"Female\"))  %>% mutate_at(vars(gender), as.factor)\n",
    "\n",
    "# boxplot\n",
    "cah  %>% ggplot(aes(x = gender, y = income/1000, fill = gender)) +\n",
    "            geom_boxplot() +\n",
    "            labs(x = \"Gender\",\n",
    "                 fill = \"Gender\",\n",
    "                 y = \"Income in $1000s\",\n",
    "                 title = \"Income by Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be a difference here, but let's fit the model and see.\n",
    "\n",
    "[Return to Top](#top)\n",
    "<a id = \"modfit2\"></a>\n",
    "\n",
    "### Model Fitting\n",
    "Because gender is categorical, \"dummy\" coding is used to enter it into the model.  These are 0/1 variables that indicate whether the respondent belongs to each level of the categorical variable.  We always have one fewer \"dummy\" variables than levels of our categorical predictor.  Because gender has only two levels, we have only one parameter for gender.  R automatically sets the first factor level as the reference category to which the other parameters are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod4 <- lm(income ~ gender, data = cah)\n",
    "summary(mod4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what this means.\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "#### The intercept:\n",
    "In this model with a single categorical predictor, the intercept is the mean of income for the reference category (in this case Females).  So the average income for females in the sample is about $47k.\n",
    "\n",
    "#### The coefficient estimte for genderMale:\n",
    "Looking first at the p-value (last column) we can see that p < alpha (0.05), therefore we reject null.  This means that the coefficient for gender is significantly different from zero.  Gender is a significant predictor of income.\n",
    "\n",
    "Because the variable is categorical, genderMale is like an on/off switch.  So we cannot interpret this coefficient the same way we would for a continuous/numerical predictor. Here, we would say as compared to the reference group (Female), Males earn, on average, $11,570 more than females.\n",
    "\n",
    "#### F-statistic\n",
    "The F-statistic is our overall (omnibus) test of model fit.  It tells us:\n",
    "1. If our r-squared is significantly different from 0. AND\n",
    "2. If our model is significantly better than a \"null\" / \"empty\" / intercept only model.\n",
    "\n",
    "For this model our p-value is less than alpha, indicating that this model fits better than a null or empty model.\n",
    "\n",
    "*Because we have only one predictor variable (parameter), the p-value for the t-test of the coefficient and for the F-test of the model are identical.*\n",
    "\n",
    "#### R-squared\n",
    "R-squared is 0.02, which is very low, indicating that gender, while significant, does not explain much of the variance in income.  Because the F-test was significant, however, we do know that this r-squared is significantly different from zero.\n",
    "\n",
    "[Return to Top](#top)\n",
    "<a id = \"assump1\"></a>\n",
    "\n",
    "### Checking Assumptions\n",
    "\n",
    "#### Linear in the Parameters / Errors are Independent\n",
    "The assumption of linear in the parameters says that the variables are entered into the model correctly.  We've entered gender as a single dummy variable (genderMale = 1 is Male, genderMale = 0 is Female).  \n",
    "\n",
    "We need to check if we have independent and identically distributed (i.i.d) errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-PQ / quick and dirty plot\n",
    "# note - we're plotting with our saved model output - mod4\n",
    "plot(mod4, which = 1) ## residuals vs. fitted is plot #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fit/ diagnostic plots for models with just categorical predictors are a bit weird compared to those for numerical predictors.  Our x variable can only take two values, 0 or 1, so we just get two columns of observations.\n",
    "\n",
    "Because the red line is horizontal, we can conclude that our errors are independent.\n",
    "\n",
    "#### Right variables \n",
    "Like linear in the parameters, it relies on the analyst to make sure they're using the right variables in the model.\n",
    "\n",
    "#### Normally Distributed Errors\n",
    "This we're already familiar with from ANOVA - we look at the normality of the distribution of the residuals using a QQ plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty QQ\n",
    "plot(mod4, which = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is some substantive deviation from normality in the tails, again because the distribution income is right-skewed.\n",
    "\n",
    "#### No influential outliers\n",
    "We need to make sure that we don't have any observations that 1. Have inordinate influence on the fit of the line and 2. are outliers.  For this week look at a plot of Residuals vs. Leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty plot\n",
    "plot(mod4, which = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashed red Cook's distance line doesn't even show on the plot, therefore we don't have any influential outliers.\n",
    "\n",
    "#### Homoscedasticity / Homoskedasticity\n",
    "Finally, we turn to our last assumption, Homoscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty resid/pred and scale location \n",
    "plot(mod4, which = c(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red reference line in both plots appears to be relatively horizontal, therefore I don't anticipate that we have heteroscedasticity.  We are not violating the assumption of constant variance.\n",
    "\n",
    "### Effect Size\n",
    "\n",
    "#### Unstandardized\n",
    "The difference in mean income is about $11k, which I think is substantive.\n",
    "\n",
    "#### Standardized\n",
    "We can't standardize gender (it's 0/1) but we can standardize income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cah3 <- cah %>% mutate(income_std = scale(income))\n",
    "\n",
    "mod5 <- lm(income_std ~ gender, data = cah3)\n",
    "summary(mod5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean income for males is a third of a standard deviation higher than the overall (grand) mean of income.  The mean income for females (intercept), is 0.16 of a SD below the grand (overall) mean.\n",
    "\n",
    "### Isn't this just a two-sample t-test?\n",
    "You may not have been thinking this, but this linear model, with one two-level categorical variable is **EXACTLY THE SAME** as conducting a two-sample t-test of income by gender.  \n",
    "\n",
    "Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(mod4)\n",
    "t.test(income ~ gender, data = cah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And look at that - \n",
    "\n",
    "1. The intercept of our linear model is the same thing as the mean in group Female in our t-test output.\n",
    "2. The t-score and p-values are nearly identical (the t-test uses the Welch's adjustment)\n",
    "\n",
    "Almost everything in statistics is a linear model \"underneath the hood.\" \n",
    "https://twitter.com/ChelseaParlett/status/1249112302348980226?s=20\n",
    "\n",
    "[Return to Top](#top)\n",
    "<a id = \"perf\"></a>\n",
    "\n",
    "## OPTIONAL - Model Fit Dashboard using `performance`\n",
    "\n",
    "The `performance` package aims to make model diagnostics easy to generate.  \n",
    "https://easystats.github.io/performance/\n",
    "\n",
    "Let's look at some of the functions they offer.  We'll look at our models using age and agesq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance, single model\n",
    "model_performance(mod2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the model performance measures, including r-squared and adjusted r-squared.\n",
    "\n",
    "We can compare two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare performance\n",
    "compare_performance(mod1, mod2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can get our diagnostic plots as one large \"dashboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model(mod1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ignore the warnings - we can't check for multicollinearity because there's only one predictor.  But overall this gives us a nice concise overview of all of the diagnostic plots we need to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check heteroscedasticity (B-P test)\n",
    "check_heteroscedasticity(mod1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check normality of residuals using sig test (not typically preferred over QQ plot due to sensitivity)\n",
    "# Statistical test is the Shapiro-Wilk Normality Test \n",
    "check_normality(mod1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Like most statistical significance tests, if the sample size is sufficiently large this test may detect even trivial departures from the null hypothesis (i.e., although there may be some statistically significant effect, it may be too small to be of any practical significance); thus, additional investigation of the effect size is typically advisable, e.g., a Qâ€“Q plot in this case.\" https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
